<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Spoken English App</title>
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
  <style>
    .dialog-box {
      border: 1px solid #ccc;
      border-radius: 10px;
      padding: 15px;
      margin-top: 20px;
    }
    .dialog-app, .dialog-me {
      padding: 10px;
      margin-bottom: 10px;
    }
    .dialog-app {
      background-color: #f8f9fa;
    }
    .dialog-me {
      background-color: #e9ecef;
    }
    .highlight {
      background-color: yellow;
    }
  </style>
</head>
<body>
  <div class="container mt-5">
    <h1 class="text-center">Spoken English App</h1>
    <div class="text-center mt-4">
      <button id="startConversation" class="btn btn-primary">Start Conversation</button>
      <button id="endConversation" class="btn btn-danger" onclick="endConversation()">End Conversation</button>
    </div>
    <div id="dialogContainer" class="dialog-box mt-4">
      <!-- Dialog will appear here -->
    </div>
  </div>

  <script>
    document.getElementById('startConversation').addEventListener('click', startConversation);

    const dialogScript = [
      { app: "Hello! How are you today?", me: "I am fine, thank you." },
      { app: "What is your name?", me: "My name is John." },
      // Add more dialog lines as needed
    ];

    let currentStep = 0;
    let recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    let synth = window.speechSynthesis;

    let mediaRecorder;
    let recordedChunks = [];

    navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
      mediaRecorder = new MediaRecorder(stream);

      mediaRecorder.ondataavailable = function (event) {
        if (event.data.size > 0) {
          recordedChunks.push(event.data);
        }
      };

      mediaRecorder.onstop = function () {
        const blob = new Blob(recordedChunks, { type: 'audio/webm' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.style.display = 'none';
        a.href = url;
        a.download = 'conversation.webm';
        document.body.appendChild(a);
        a.click();
        window.URL.revokeObjectURL(url);
      };
    });

    function startConversation() {
      mediaRecorder.start();
      displayDialogScript();
      highlightCurrentStep();
      if (currentStep < dialogScript.length) {
        speakText(dialogScript[currentStep].app, promptUserToSpeak);
      }
    }

    function displayDialogScript() {
      const dialogContainer = document.getElementById('dialogContainer');
      dialogContainer.innerHTML = '';
      dialogScript.forEach((line, index) => {
        const appDiv = document.createElement('div');
        appDiv.className = `dialog-app${index === currentStep ? ' highlight' : ''}`;
        appDiv.innerHTML = `<strong>App:</strong> ${line.app}`;
        dialogContainer.appendChild(appDiv);

        const meDiv = document.createElement('div');
        meDiv.className = `dialog-me${index === currentStep ? ' highlight' : ''}`;
        meDiv.innerHTML = `<strong>Me:</strong> ${line.me}`;
        dialogContainer.appendChild(meDiv);
      });
    }

    function highlightCurrentStep() {
      const dialogContainer = document.getElementById('dialogContainer');
      const children = dialogContainer.children;
      for (let i = 0; i < children.length; i++) {
        children[i].classList.remove('highlight');
        if (i === currentStep * 2 || i === currentStep * 2 + 1) {
          children[i].classList.add('highlight');
        }
      }
    }

    function speakText(text, callback) {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.onend = function () {
        if (callback) callback();
      };
      synth.speak(utterance);
    }

    function promptUserToSpeak() {
      const dialogContainer = document.getElementById('dialogContainer');
      const promptDiv = document.createElement('div');
      promptDiv.className = 'alert alert-info mt-2';
      promptDiv.innerHTML = 'It\'s your turn to speak.';

      dialogContainer.appendChild(promptDiv);

      recognition.start();

      recognition.onresult = function (event) {
        const userSpeech = event.results[0][0].transcript;
        showDialog('me', userSpeech);
        recognition.stop();
        dialogContainer.removeChild(promptDiv);

        // Move to the next step
        currentStep++;
        highlightCurrentStep();
        if (currentStep < dialogScript.length) {
          speakText(dialogScript[currentStep].app, promptUserToSpeak);
        } else {
          // Conversation ended
          alert('Conversation ended!');
        }
      };

      recognition.onerror = function () {
        recognition.stop();
      };
    }

    function showDialog(role, text) {
      const dialogContainer = document.getElementById('dialogContainer');
      const dialogDiv = document.createElement('div');
      dialogDiv.className = `dialog-${role}`;
      dialogDiv.innerHTML = `<strong>${role === 'app' ? 'App' : 'Me'}:</strong> ${text}`;
      dialogContainer.appendChild(dialogDiv);
    }

    function endConversation() {
      mediaRecorder.stop();
      alert('Conversation recorded and ready to download.');
    }
  </script>
</body>
</html>
